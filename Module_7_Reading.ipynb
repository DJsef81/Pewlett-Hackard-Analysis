{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES FOR MODULE 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2.2 Create Tables in SQL\n",
    "\n",
    "# Use our final ERD as a guide\n",
    "\n",
    "# create six tables, one for each CSV file\n",
    "\n",
    "# These table creation statements will be our first introduction to Structured Query Language. \n",
    "\n",
    "# A statement is a block of code that, when executed, sends a command to the database.\n",
    "\n",
    "#  Before we create database, review Diagrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review Diagrams\n",
    "\n",
    "# recreating the same tables in the diagram in our SQL database.\n",
    "\n",
    "# With the help of the diagram, we know the structure of this table: two columns with their data types. \n",
    "# Also, the table is already named. All we need to do is transfer over the same information.\n",
    "\n",
    "# Start by using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Query Tool is pgAdmin's text editor, much like VSCode is for Python.\n",
    "\n",
    "# so lets create a table\n",
    "\n",
    "# -- Creating tables for PH-EmployeeDB\n",
    "# CREATE TABLE departments (\n",
    "#      dept_no VARCHAR(4) NOT NULL,\n",
    "#     dept_name VARCHAR(40) NOT NULL,\n",
    "#     PRIMARY KEY (dept_no),\n",
    "#     UNIQUE (dept_name)\n",
    "#);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE is the syntax required to create a new table in SQL.\n",
    "\n",
    "# departments is the name of the table and how it will be referenced in queries.\n",
    "\n",
    "# So the table has been named, now the structure needs to be created. The content inside the parentheses is how we'll \n",
    "# do that.\n",
    "\n",
    "# dept_no VARCHAR(4) NOT NULL, creates a column named \"dept_no\" that can hold up to four varying characters, while \n",
    "# NOT NULL tells SQL that no null fields will be allowed when importing data.\n",
    "\n",
    "# There are times when we don't want a data field to be null. For example, the dept_no column is our primary key—each \n",
    "# row has a unique number associated with it. If we didn't have the NOT NULL constraint, then there's a chance that a \n",
    "# row (or more than one row) won't have a primary key associated with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you think would happen if one of the rows didn’t have a unique identifier?\n",
    "\n",
    "# Not all of the data would be present in every query, which would skew analysis results and provide incomplete lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dept_name VARCHAR(40) NOT NULL, creates a column similar to the dept_no, only the varying character count has a \n",
    "# maximum of 40.\n",
    "\n",
    "# PRIMARY KEY (dept_no), means that the dept_no column is used as the primary key for this table.\n",
    "\n",
    "# UNIQUE (dept_name) adds the unique constraint to the dept_name column.\n",
    "\n",
    "# The unique constraint implies that the data in that column is unique. \n",
    "# This ensures that if the table were to be updated in the future, nothing will be duplicated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The closing parenthesis and semicolon signal that the SQL CREATE TABLE statement is complete. \n",
    "\n",
    "# Any code added after will need to be included in a new SQL statement. \n",
    "# A statement is a command that is set up with a certain syntax. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does it mean when the NOT NULL constraint is applied?\n",
    "\n",
    "# Null values are not allowed in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the Code\n",
    "\n",
    "# To save the table to the database, we need to execute the code. \n",
    "\n",
    "# In the toolbar of the pgAdmin webpage, find and click the lightning bolt symbol toward the right of the bar. \n",
    "\n",
    "# This button runs the code and saves our work to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshoot Error Messages\n",
    "\n",
    "# Any error message will also appear in the same manner. Encountering errors will happen often and troubleshooting \n",
    "# them is a large part of being a developer. \n",
    "\n",
    "# Thankfully, each error message encountered will tell us why the error occurred. \n",
    "# This is great because it helps us, the developers, research and fix the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR:  relation \"departments\" already exists\n",
    "# SQL state: 42P07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This error occurs because SQL data is persistent and cannot be overwritten if the same command is run again. \n",
    "# Once a table has been committed to a database, it is there until a different command is run to delete it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data integrity is the quality of the data we're working with. Clean data will yield better results in analysis, \n",
    "# and maintaining the data integrity ensures greater accuracy and reliability.\n",
    "\n",
    "# Dirty data is data that contains errors such as duplicates, undefined values (i.e., not a number, or NaN), or other \n",
    "# inconsistencies. This is why the NOT NULL constraint is in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid encountering this error, highlight the code block you want to run first, then execute it. \n",
    "# This tells pgAdmin to run only that code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Additional Tables\n",
    "\n",
    "# Create another table for Employees.\n",
    "\n",
    "# How to start create table: \n",
    "\n",
    "# CREATE TABLE employees (\n",
    "#     emp_no INT NOT NULL,\n",
    "#     birth_date DATE NOT NULL,\n",
    "#     first_name VARCHAR NOT NULL,\n",
    "#     last_name VARCHAR NOT NULL,\n",
    "#     gender VARCHAR NOT NULL,\n",
    "#     hire_date DATE NOT NULL,\n",
    "#     PRIMARY KEY (emp_no)\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one more together—this time with foreign keys included. \n",
    "# Add the following code to the bottom of your query editor:\n",
    "\n",
    "# CREATE TABLE dept_manager (\n",
    "# dept_no VARCHAR(4) NOT NULL,\n",
    "#     emp_no INT NOT NULL,\n",
    "#     from_date DATE NOT NULL,\n",
    "#     to_date DATE NOT NULL,\n",
    "# FOREIGN KEY (emp_no) REFERENCES employees (emp_no),\n",
    "# FOREIGN KEY (dept_no) REFERENCES departments (dept_no),\n",
    "#     PRIMARY KEY (emp_no, dept_no)\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that foreign keys reference the primary key of other tables. In the two lines above we can see that:\n",
    "\n",
    "# The FOREIGN KEY constraint tells Postgres that there is a link between two tables\n",
    "\n",
    "# The parentheses following FOREIGN KEY specify which of the current table's columns is linked to another table\n",
    "\n",
    "# REFERENCES table_name (column_name) tells Postgres which other table uses that column as a primary key\n",
    "\n",
    "# The primary key is similar, but there are two keys listed this time instead of just one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One thing to keep in mind when working with foreign keys is that it's possible that data insertion will fail if the\n",
    "# foreign key isn't present. \n",
    "\n",
    "# This is a \"foreign key constraint\" and in this case, it means that the new data needs a reference point \n",
    "# (such as dept_no or emp_no) to be successfully added to the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create another table for the data in salaries.csv\n",
    "\n",
    "# CREATE TABLE salaries (\n",
    "#   emp_no INT NOT NULL,\n",
    "#   salary INT NOT NULL,\n",
    "#   from_date DATE NOT NULL,\n",
    "#   to_date DATE NOT NULL,\n",
    "#   FOREIGN KEY (emp_no) REFERENCES employees (emp_no),\n",
    "#   PRIMARY KEY (emp_no)\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code tells Postgres that our new table is named \"salaries\" and we'll have columns for the emp_no, salary, \n",
    "#from_date, and to_date. \n",
    "\n",
    "# We also have specified that certain fields aren't allowed any null space with the NOT NULL constraint, which is \n",
    "# important because we want this data to be persistent for every employee. \n",
    "\n",
    "# As a final step in table creation, we've also specified primary and foreign keys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for Confirmation\n",
    "\n",
    "# Confirm the tables were created successfully by running a SELECT statement, which performs a query instead of \n",
    "# constructing anything.\n",
    "\n",
    "# Think of it as asking the database a question. For example, say we want to know how many columns are in the \n",
    "# departments table. \n",
    "# How would we ask that particular question? We would create a SELECT statement, then run the code. \n",
    "# This is called \"querying the database.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to query how many columns ar in the departments table\n",
    "\n",
    "# In the editor, after the table creation statements, type \n",
    "\n",
    "# SELECT * FROM departments;\n",
    "\n",
    "# The SELECT statement tells Postgres that we're about to query the database.\n",
    "# The asterisk tells Postgres that we're looking for every column in a table.\n",
    "# FROM departments tells pgAdmin which table to search.\n",
    "# The semicolon signifies the completion of the query.\n",
    "# After executing the SELECT statement, pgAdmin will automatically show the result in the Data Output tab at the bottom of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information in the database is static, which means that it will always be in the database unless directly \n",
    "# altered, but the query editor is not. \n",
    "\n",
    "# It's similar to working in a Microsoft Word document: If something happens to your computer before you saved your \n",
    "# work, there's a good chance it'll be lost. \n",
    "\n",
    "# If your computer crashes mid-query, the pgAdmin editor won't hold onto your code for you during a reboot.\n",
    "\n",
    "# Our queries are the meat and potatoes of SQL. We're finding connections between different tables and answering \n",
    "# questions with the results. \n",
    "\n",
    "# Even though losing a query isn't the end of the world (they can be rebuilt, after all), it does take time to \n",
    "# replicate the work already completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2.3 Import Data\n",
    "\n",
    "# we've created a database. We've written our first SQL code and created tables modeled after our ERD. \n",
    "\n",
    "# The next step is to import the data from the CSV files. \n",
    "\n",
    "# We'll make sure all of the tables we created in pgAdmin appear in the GUI first, because we'll be using the GUI to \n",
    "# import data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL is very interactive. Developers are not only importing data and asking it questions through the query language, \n",
    "# but they can also update and edit the data stored in the tables as needed.\n",
    "\n",
    "# For example, if a single row of data needs to be added to an existing table, a developer can manually add it by \n",
    "# using the INSERT statement.\n",
    "\n",
    "# If the data in a table is small enough in scale, it can be manually inserted this way completely, instead of \n",
    "# importing a CSV file.\n",
    "\n",
    "# Alternately, necessary edits and updates are completed manually as well. \n",
    "\n",
    "# We won't be manually editing or uploading data to our tables in this lesson because our datasets are too large.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep to import csv files into PH-EmployeeDB\n",
    "\n",
    "# In the pgAdmin window, select the dropdown menu for our PH-EmployeeDB database. To import data into the tables, first confirm all of our tables are listed:\n",
    "\n",
    "# Find the PH-EmployeeDB collapsible menu and click it.\n",
    "# Scroll down and click \"Schemas\" to expand the menu.\n",
    "# Click \"public.\"\n",
    "# Scroll down to \"Tables\" and note the number in parentheses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Import \n",
    "\n",
    "# To import a CSV into Postgres with pgAdmin, follow these steps. We'll customize our options to fit our data import, \n",
    "# and then check the table to make sure the data has been imported successfully. \n",
    "\n",
    "# Right-click the first table, departments.\n",
    "\n",
    "# From the menu that pops up, scroll to Import/Export. \n",
    "\n",
    "# Toggle the button to show \"Import.\" \n",
    "\n",
    "# Click the ellipsis on the Filename field to search for your project folder.\n",
    "\n",
    "# Select departments.csv. Make sure Format is set to \"csv\" and Encoding is blank. Note: By default, the Encoding \n",
    "# section is blank. \n",
    "# If our files were encoded to provide an extra layer of security, we would need to select the type of encoding before\n",
    "# importing them to Postgres. \n",
    "# We don't have to worry about this, though. \n",
    "# Also, if \"Encoding\" is filled in with an encoding type such as BIG5 or LATIN1, cancel the import and start over. \n",
    "\n",
    "# Leave the OID field as is, but toggle the Header field to \"Yes\" and select the comma as the Delimiter. \n",
    "# Note: If we don't specify that there is already a header included in the CSV data, then the header will be imported \n",
    "# as data. This would result in errors because headers don't always match the data types in the columns. \n",
    "\n",
    "# Click OK to begin importing the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the import by typing SELECT * FROM departments; at the bottom of the query editor. \n",
    "# The resulting table should mirror the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2.4 Troubleshoot Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Common Errors\n",
    "\n",
    "# What if Bobby runs into an error while he's importing the data? Below is an example of a likely error:\n",
    "\n",
    "# DETAIL: Key(Emp_no)=10001 is not present in table \"employees\"\n",
    "\n",
    "# Because the FOREIGN KEY constraint references other tables, we need to import the data in a specific order.\n",
    "\n",
    "# For example, the dept_emp table references the Employees table through its foreign key. \n",
    "# If there is no data in the Employees table, then there are no foreign keys to link to, and an error will occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Mismatched Data Types\n",
    "\n",
    "# Another common scenario is when a data type in a table we've created doesn't match the CSV data. What should we do?\n",
    "\n",
    "# Because data within a Postgres database is static, we can't go back and fix a typo in our original table creation \n",
    "# code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to update a table column to fix its data type, what is the best approach?\n",
    "# Delete the table, then recreate it after updating the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a Table\n",
    "\n",
    "# To drop the table, the following code is used:\n",
    "\n",
    "# DROP TABLE employees CASCADE;\n",
    "\n",
    "# DROP TABLE employees tells Postgres that we want to remove the Employees table from the database completely.\n",
    "# CASCADE; indicates that we also want to remove the connections to other tables in the database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More on CASCADE\n",
    "\n",
    "# Even without data, by adding foreign keys that reference other tables, we've created a network of data connections. \n",
    "\n",
    "# Not every table will need the CASCADE constraint, but it will come up when you need to drop a table that already has \n",
    "# a defined relationship with another. \n",
    "\n",
    "# Any table that does not reference a foreign key can be dropped without the CASCADE constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3.1 Query Dates\n",
    "\n",
    "# future-proofing the company by determining how many people will be retiring and, of those employees, who is eligible \n",
    "# for a retirement package.\n",
    "\n",
    "# In Python, conditionals such as \"if\" and \"else,\" and the logical operator \"and,\" are similar to conditional \n",
    "# expressions used in SQL\n",
    "\n",
    "# search for folks who are retiring soon. The query he builds will include a condition involving employee birthdays. \n",
    "# We need to know when they were born to determine when they'll retire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Retirement Eligibility\n",
    "\n",
    "# anyone born between 1952 and 1955 will begin to retire. \n",
    "\n",
    "# The first query we need to help Bobby write will return a list of those employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT first_name, last_name\n",
    "# FROM employees\n",
    "# WHERE birth_date BETWEEN '1952-01-01' AND '1955-12-31';\n",
    "\n",
    "\n",
    "# The SELECT statement is more specific this time. Instead of an asterisk to indicate that we want all of the records,\n",
    "# we're requesting only the first and last names of the employees.\n",
    "\n",
    "# FROM employees tells SQL in which of the six tables to look.\n",
    "\n",
    "# The WHERE clause brings up even more specifics. We want SQL to look in the birth_date column for anyone born between\n",
    "# January 1, 1952, and December 31, 1955.\n",
    "\n",
    "# Notice how BETWEEN and AND are both capitalized in our statement? This is part of the SQL syntax. \n",
    "# It not only signals the conditions present, but also makes the code easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many employees are ready for retirement?\n",
    "\n",
    "# 10,000 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pewlett Hackard has a lot of employees getting ready to age out of the program. \n",
    "# This is going to create a considerable amount of openings. Refine this list further by looking only at how many \n",
    "# employees were born in 1952. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another query that will search for only 1952 birth dates.\n",
    "\n",
    "# SELECT first_name, last_name\n",
    "# FROM employees\n",
    "# WHERE birth_date BETWEEN '1952-01-01' AND '1952-12-31';\n",
    "\n",
    "# This query is almost the same as the last. We've only changed a single digit: the year was switched from 1955 to \n",
    "# 1952 after the AND clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrow the Search for Retirement Eligibility\n",
    "\n",
    "# There are quite a few folks getting ready to retire. Each of those new queries has a lengthy list of people. \n",
    "\n",
    "# Let's see if we can narrow it down a bit more by adding another condition to the query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified query \n",
    "\n",
    "# -- Retirement eligibility\n",
    "# SELECT first_name, last_name\n",
    "# FROM employees\n",
    "# WHERE (birth_date BETWEEN '1952-01-01' AND '1955-12-31')\n",
    "# AND (hire_date BETWEEN '1985-01-01' AND '1988-12-31');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We modified this query to include a specific hiring range. This time, we're looking for employees born between 1952 \n",
    "# and 1955, who were also hired between 1985 and 1988. \n",
    "\n",
    "# The modification is subtle, too. We're going to adjust one line of the code block and add another to the end.\n",
    "\n",
    "# The first piece, an adjustment, is to place parentheses around the WHERE clause (without including the keyword \n",
    "# itself). We'll also remove the semicolon since the code block isn't complete yet. \n",
    "\n",
    "# dd the final line of code. Our current code has a single condition in place that tells Postgres to search only for \n",
    "# people born between 1952 and 1955. The next line of code is our second condition that they were also hired between \n",
    "# 1985 and 1988.\n",
    "\n",
    "# the second condition is inside parentheses? This is a tuple; in Python, data can be stored inside a tuple and \n",
    "# accessed in the same way as a list. \n",
    "\n",
    "# In SQL, the tuples in this block of code are part of the syntax. They basically place each condition in a group, \n",
    "# and Postgres looks for the first group first, then looks inside the second group to deliver the second condition.\n",
    "\n",
    "# The SELECT statement, pulling data from the first and last name columns\n",
    "# The FROM statement, telling Postgres from which table we're getting the data\n",
    "# And two conditional statements: the dates of birth and the dates of hire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the Queries\n",
    "\n",
    "# -- Number of employees retiring\n",
    "# SELECT COUNT(first_name)\n",
    "# FROM employees\n",
    "# WHERE (birth_date BETWEEN '1952-01-01' AND '1955-12-31')\n",
    "# AND (hire_date BETWEEN '1985-01-01' AND '1988-12-31');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New Tables\n",
    "\n",
    "# This time, the change we'll make to the code is also small—we're modifying the SELECT statement into a SELECT INTO \n",
    "# statement. \n",
    "# This statement tells Postgres that instead of generating a list of results, the data is saved as a new table \n",
    "# completely. \n",
    "\n",
    "# Update our code to include the INTO portion of the SELECT statement.\n",
    "\n",
    "# Insert a new, blank line between the SELECT and FROM sections of the code. \n",
    "# In this vacant space, type in INTO retirement_info. With the addition of this line, we're telling Postgres to save \n",
    "# the data into a table named \"retirement_info.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- create a new table of retiring employees\n",
    "\n",
    "# SELECT first_name, last_name\n",
    "# INTO retirement_info\n",
    "# FROM employees\n",
    "# WHERE (birth_date BETWEEN '1952-01-01' AND '1955-12-31')\n",
    "# AND (hire_date BETWEEN '1985-01-01' AND '1988-12-31');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our list of data from earlier is now an actual table that we can use with statements and functions to perform \n",
    "# analysis. \n",
    "\n",
    "# Additionally, if you refresh the list of tables from the dropdown menu on the left, it will now appear in the list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Data\n",
    "\n",
    "# Right-click on your new table and select \"Import/Export.\" \n",
    "# Instead of importing anything, this time we'll be exporting.\n",
    "\n",
    "# STEPS\n",
    "\n",
    "# Keep the Import/Export button toggled to \"Export.\"\n",
    "\n",
    "# Click on the ... in the Filename field to automatically select the same directory from which you imported the other \n",
    "# CSVs. Select a directory, but be sure to rename it to retirement_info.csv.\n",
    "\n",
    "# Be sure the format is still CSV.\n",
    "\n",
    "# Toggle the Header section to \"Yes\" to include column names in the new CSV files.\n",
    "\n",
    "# Select the comma as the delimiter to maintain the same format with all CSV files.\n",
    "\n",
    "# Click OK to start the export. After the file has been created, pgAdmin will confirm our file is ready to be viewed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
